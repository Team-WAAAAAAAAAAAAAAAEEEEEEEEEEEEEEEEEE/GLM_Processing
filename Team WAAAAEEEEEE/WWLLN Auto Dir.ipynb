{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # makes the notebook fill the whole window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import ticker\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Polygon\n",
    "from sklearn.cluster import DBSCAN\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import shapefile\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import CubicSplineTrack\n",
    "#(c)2015 Robert Sedgewick, Kevin Wayne, and Robert Dondero\n",
    "from greatcircle import great_circle\n",
    "from datetime import date\n",
    "import shutil\n",
    "import imageio\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Turn off interactive plotting for pyplot\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storms Found: 35\n",
      "['Hurricane Arthur', 'Hurricane Bertha', 'Hurricane Beta', 'Hurricane Cristobal', 'Hurricane Delta', 'Hurricane Dolly', 'Hurricane Dorian', 'Hurricane Edouard', 'Hurricane Epsilon', 'Hurricane Eta', 'Hurricane Fay', 'Hurricane Five', 'Hurricane Gamma', 'Hurricane Gonzalo', 'Hurricane Hanna', 'Hurricane Iota', 'Hurricane Isaias', 'Hurricane Josephine', 'Hurricane Kyle', 'Hurricane Laura', 'Hurricane Marco', 'Hurricane Nana', 'Hurricane Omar', 'Hurricane Paulette', 'Hurricane Rene', 'Hurricane Sally', 'Hurricane Teddy', 'Hurricane Ten', 'Hurricane Theta', 'Hurricane Twenty', 'Hurricane Twenty-Eig', 'Hurricane Twenty-Nin', 'Hurricane Vicky', 'Hurricane Wilfred', 'Hurricane Zeta']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Team WAAAAAAEEEEE Cassie\"\"\"\n",
    "check_minutes = 60 # minuite var\n",
    "split_storm = 1 # seperate or no\n",
    "\n",
    "only_dirs = [d for d in listdir(\"Data/\") if isdir(join(\"Data/\", d))] # makes a list of all the folders in Data/\n",
    "dir_count = len(only_dirs)\n",
    "\n",
    "WWLLN_Track_Path = [\"\"] * dir_count # the paths of all the track files, in the same order as only_dirs\n",
    "WWLLN_Locations_Path = [\"\"] * dir_count # the paths of all the lightning locations files, in the same order as only_dirs\n",
    "Cubic_Spline_Path = [\"\"] * dir_count # the paths of all the cubic spline files, in the same order as only_dirs\n",
    "\n",
    "Image_Path = [\"\"] * dir_count # Base Image paths\n",
    "Storm_Names = [\"\"] * dir_count # Base storm names\n",
    "T_Path = [\"\"] * dir_count\n",
    "\n",
    "# loop thru all dirs\n",
    "storms_found = 0\n",
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    \n",
    "    # this gets all the file paths in a given storms' folder\n",
    "    only_dirs[dir_indx] = join(\"Data/\", only_dirs[dir_indx])\n",
    "    file_paths = listdir(only_dirs[dir_indx])\n",
    "    \n",
    "    # check if the 3 nessisary files exist\n",
    "    if len(file_paths) < 2:\n",
    "        only_dirs[dir_indx] = \"\"\n",
    "        dir_indx+=1\n",
    "        continue\n",
    "    \n",
    "    #print(only_dirs[dir_indx])\n",
    "    #print(file_paths)\n",
    "    \n",
    "    # for every file in file_paths, sort it into its' respective arrays\n",
    "    file_indx = 0\n",
    "    files_found = 0\n",
    "    Cubic_Found = False\n",
    "    while file_indx < len(file_paths):\n",
    "        # creates the files full path\n",
    "        file_paths[file_indx] = join(only_dirs[dir_indx]+\"/\", file_paths[file_indx])\n",
    "        #print(file_paths[file_indx])\n",
    "        \n",
    "        # sorts the file path based on the last expected word of a file and it's type\n",
    "        if file_paths[file_indx].endswith(\"Trackfile.txt\"):\n",
    "            WWLLN_Track_Path[dir_indx] = file_paths[file_indx]\n",
    "            files_found+=1\n",
    "            \n",
    "            # generates the name of the storm for images\n",
    "            Storm_Names[dir_indx] = \"Hurricane \" + WWLLN_Track_Path[dir_indx].rsplit(\"_\", 3)[1]\n",
    "            \n",
    "        elif file_paths[file_indx].endswith(\"Locations.txt\"):\n",
    "            WWLLN_Locations_Path[dir_indx] = file_paths[file_indx]\n",
    "            files_found+=1\n",
    "            \n",
    "        elif file_paths[file_indx].endswith(\"Trackfile.csv\"):\n",
    "            Cubic_Spline_Path[dir_indx] = file_paths[file_indx]\n",
    "            Cubic_Found = True\n",
    "            files_found+=1\n",
    "        #print(f\"File: {file_paths[file_indx]}, Storm indx: {dir_indx}\\n\")\n",
    "        file_indx+=1\n",
    "    \n",
    "    # Run with Bens' Cubic Spline\n",
    "    if not Cubic_Found:\n",
    "        Cubic_Spline_Path[dir_indx] = join(only_dirs[dir_indx]+\"/\", Storm_Names[dir_indx] + \" Cubic Spline Trackfile.csv\")\n",
    "        #print(WWLLN_Track_Path[dir_indx])\n",
    "        CubicSplineTrack.cubic_spline_trackfile(WWLLN_Track_Path[dir_indx], Cubic_Spline_Path[dir_indx], check_minutes)\n",
    "        files_found+=1\n",
    "    \n",
    "    if files_found != 3:\n",
    "        print(f\"Incorrect files found: {files_found}, {file_paths}\\n\")\n",
    "        only_dirs[dir_indx] = \"\"\n",
    "        dir_indx+=1\n",
    "        continue\n",
    "    \n",
    "    # makes the path for this index's storm\n",
    "    Image_Path[dir_indx] = join(\"./Images/\", Storm_Names[dir_indx])\n",
    "    \n",
    "    if not os.path.exists(Image_Path[dir_indx]):\n",
    "        os.mkdir(Image_Path[dir_indx])\n",
    "    \n",
    "    T_Path[dir_indx] = Image_Path[dir_indx] + \"/T Value\"\n",
    "    Image_Path[dir_indx] = Image_Path[dir_indx] + \"/\" + Storm_Names[dir_indx]\n",
    "    \n",
    "    if split_storm:\n",
    "        Image_Path[dir_indx] = Image_Path[dir_indx] + \" Split \" + str(check_minutes) +\" Minutes\"\n",
    "    else:\n",
    "        Image_Path[dir_indx] = Image_Path[dir_indx] + \" \" + str(check_minutes) +\" Minutes\"\n",
    "    \n",
    "    # if an image folder |dosen't| exist, create a new one with the path at Image_Path[dir_indx]\n",
    "    if not os.path.exists(Image_Path[dir_indx]):\n",
    "        os.mkdir(Image_Path[dir_indx])\n",
    "    \n",
    "    Image_Path[dir_indx] = Image_Path[dir_indx] + '/' # add a slash for use in the next block, making the actual img paths\n",
    "    \n",
    "    if not os.path.exists(T_Path[dir_indx]):\n",
    "        os.mkdir(T_Path[dir_indx])\n",
    "        T_Path[dir_indx] = T_Path[dir_indx] + '/'\n",
    "        if not os.path.exists(T_Path[dir_indx]+\"0.0\"):\n",
    "            os.mkdir(T_Path[dir_indx]+\"0.0\")\n",
    "        T_value = 1.5\n",
    "        while T_value < 9:\n",
    "            if not os.path.exists(T_Path[dir_indx]+str(T_value)):\n",
    "                os.mkdir(T_Path[dir_indx]+str(T_value))\n",
    "            T_value+=0.5\n",
    "    else:\n",
    "        T_Path[dir_indx] = T_Path[dir_indx] + '/'\n",
    "    \n",
    "    #print('\\n')\n",
    "    storms_found+=1\n",
    "    dir_indx+=1\n",
    "\n",
    "if not os.path.exists(\"Images/T Value\"):\n",
    "    os.mkdir(\"Images/T Value\")\n",
    "    if not os.path.exists(\"Images/T Value/0.0\"):\n",
    "        os.mkdir(\"Images/T Value/0.0\")\n",
    "    T_value = 1.5\n",
    "    while T_value < 9:\n",
    "        if not os.path.exists(\"Images/T Value/\"+str(T_value)):\n",
    "            os.mkdir(\"Images/T Value/\"+str(T_value))\n",
    "        T_value+=0.5\n",
    "\n",
    "# print for debug\n",
    "print(f\"Storms Found: {storms_found}\")\n",
    "print(Storm_Names)\n",
    "#print(Image_Path)\n",
    "#print(WWLLN_Track_Path)\n",
    "#print(WWLLN_Locations_Path)\n",
    "#print(Cubic_Spline_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAEEEEEEEEE Lance\"\"\"\n",
    "def run_circle(row, center_lat, center_lon): \n",
    "    lat = (row['Lat'])\n",
    "    lon = (row['Long'])\n",
    "    return great_circle(lat, lon, center_lat, center_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAAEEEEEEEEEE Bhargav\"\"\"\n",
    "#Creating intensity table\n",
    "intensity_values = {\n",
    "     'T-Number': [1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5],\n",
    "  '1-min Winds': [25, 30, 35, 45, 55, 65, 77, 90, 102, 115, 127, 140, 155, 170, 185],\n",
    "'Min. Pressure': [0, 1009, 1005, 1000, 994, 987, 979, 970, 960, 948, 935, 921, 906, 890, 873]\n",
    "}\n",
    "T_frame = pd.DataFrame(intensity_values)\n",
    "\n",
    "def Tvalue(wind_speed):\n",
    "    i=0\n",
    "    half=0\n",
    "    current = 0\n",
    "    prev=0\n",
    "    after=0\n",
    "    for i in range(15):\n",
    "        current = T_frame.loc[i, '1-min Winds']\n",
    "        if (i == 0):\n",
    "            prev=20\n",
    "        else:\n",
    "            prev = T_frame.loc[(i-1), '1-min Winds']\n",
    "        \n",
    "        if(i==14):\n",
    "            after = 188\n",
    "            \n",
    "        else:\n",
    "            after=T_frame.loc[(i+1), '1-min Winds']\n",
    "\n",
    "        #if(wind_speed < 25):\n",
    "            #return 1.5\n",
    "        \n",
    "        if(wind_speed > 185):\n",
    "            return 8.5\n",
    "        \n",
    "        else:\n",
    "            half_prev =current - ((current-prev)/2)\n",
    "            half_aft = current + ((after-prev)/2)\n",
    "            if(wind_speed >= half_prev and wind_speed < half_aft):\n",
    "                return T_frame.loc[i, 'T-Number']\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm #6: Hurricane Dorian, T3.0\tPercent Done:  100.00%\tTime taken:  0.54 seconds\tEst time remaining: 0:00:00 05523 0574\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "-cite sources (great circle etc.)\n",
    "-sort output by intensity (Dvorac scale?) 1-5\n",
    "-statitics over a single storm and multiple storms\n",
    "-faster file save with PIL?\n",
    "\"\"\"\n",
    "# for every storm, do the things\n",
    "\"\"\"Team WAAAAAAAEEEEEE Cassie\"\"\"\n",
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    # if storm is invalid, skip doin the things\n",
    "    if only_dirs[dir_indx] == \"\":\n",
    "        dir_indx+=1\n",
    "        continue\n",
    "    \n",
    "    \"\"\"Team Squeem\"\"\"\n",
    "    # This loads the WWLLN trackfile Jeremy and Natalia give us\n",
    "    # wind = nautical mph, pressure = millibars\n",
    "    df = pd.read_csv(WWLLN_Track_Path[dir_indx], header=None, names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Lat\",\"Long\",\"Min_Pressure\",\"Max_Winds\",\"Unused\"], low_memory=False, sep='\\t')\n",
    "    df = df.drop(\"Unused\", axis=1)#data filtering\n",
    "    df['Date'] = df.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}\", format=\"%Y-%m-%d-%H\"), axis=1)\n",
    "    \n",
    "    \"Team WAAAAAEEEEE Lance\"\n",
    "    #wind pressure stuff \n",
    "    #run pressure wind speed function on df for every row, save into array\n",
    "    Tindex = 0\n",
    "    Tvalues = df['Max_Winds'].apply(Tvalue)\n",
    "    #Tvalues = [Tvalue(t) for t in df['Max_Winds'] if isdir(join(\"Data/\", d))]\n",
    "    #make dataframe \n",
    "    dates = df['Date']\n",
    "    #print(Tvalues)\n",
    "    #print(df['Max_Winds'])\n",
    "    if len(Tvalues) != len(dates):\n",
    "        raise Exception(\"Lengths differ!\")\n",
    "    #loop through df, for every row, check date \n",
    "    df_length = len(dates)\n",
    "    left_half_dates = [date.min] * df_length\n",
    "    right_half_dates = [date.min] * df_length\n",
    "    date_index = 1 \n",
    "    left_half_dates[0] = date.min\n",
    "    right_half_dates[df_length - 1] = date.max\n",
    "    while date_index < df_length: \n",
    "        left_half_dates[date_index] = dates.iloc[date_index - 1] + ((dates.iloc[date_index] - dates.iloc[date_index - 1]) / 2)\n",
    "        date_index += 1\n",
    "    date_index = 0\n",
    "    while date_index < df_length - 1: \n",
    "        right_half_dates[date_index] = dates.iloc[date_index] + ((dates.iloc[date_index] - dates.iloc[date_index + 1]) / 2)\n",
    "        date_index += 1\n",
    "    \n",
    "    \"\"\"Team Squeem\"\"\"\n",
    "    # This loads the WWLLN lighting location data that Jeremy and Natalia give us\n",
    "    ln = pd.read_csv(WWLLN_Locations_Path[dir_indx], header=None, names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Min\",\"Sec\",\"Lat\",\"Long\",\"Dist_East_West\",\"Dist_North_South\"], low_memory=False, sep=' ')\n",
    "    ln['Date'] = ln.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}-{int(x['Min'])}-{x['Sec']}\", format=\"%Y-%m-%d-%H-%M-%S.%f\"), axis=1)#is a data clean\n",
    "\n",
    "    # This opens the cubic spline trackfiles that Ben creates\n",
    "    storm_center = pd.read_csv(Cubic_Spline_Path[dir_indx])\n",
    "    storm_center['Date'] = storm_center['Date'].apply(pd.to_datetime)\n",
    "\n",
    "    # Genereates the list of dates from the WWLLN trackfile of a (check_minutes) range\n",
    "    dates = pd.date_range(df['Date'][0], df['Date'][len(df)-1], freq=str(check_minutes)+\"T\")\n",
    "    \n",
    "    buffer = 10\n",
    "    if split_storm:\n",
    "        buffer = 6 # affects map zooooooooooooom\n",
    "    shp_file = \"World_Countries__Generalized_-shp/World_Countries__Generalized_\" # Map file ???????\n",
    "    cmap = 'gist_ncar' # dunno WHAT this is\n",
    "\n",
    "    # Counter and iteration\n",
    "    for index, date in enumerate(dates): # this loop makes one image for every (check_minutes) amount of time\n",
    "        starttime = time.perf_counter() # speeed timer start\n",
    "        center = storm_center[storm_center['Date'] == date] # storm center at this time frame\n",
    "        \"\"\"Team WAAAAAAEEEEE Cassie and Lance\"\"\"\n",
    "        if split_storm:\n",
    "            ln_curr_timeframe = ln[(ln['Date'] >= date) & (ln['Date'] < date + datetime.timedelta(minutes=check_minutes))]\n",
    "            #print(ln_curr_timeframe)\n",
    "            if len(ln_curr_timeframe) != 0:\n",
    "                ln_curr_timeframe['Distance'] = ln_curr_timeframe.apply(run_circle, axis = 1, args = ((center['Lat']), (center['Long'])))\n",
    "                #print(ln_curr_timeframe)\n",
    "                subset = ln_curr_timeframe[(ln_curr_timeframe['Distance'] <= 100 / 1.852) | ((ln_curr_timeframe['Distance'] >= 200 / 1.852) & (ln_curr_timeframe['Distance'] <= 400 / 1.852))]\n",
    "            else:\n",
    "                subset = ln_curr_timeframe\n",
    "        else:\n",
    "            # Select just the lightning locations that are within (check_minutes) minutes\n",
    "            subset = ln[(ln['Date'] >= date) & (ln['Date'] < date + datetime.timedelta(minutes=check_minutes))]\n",
    "        \n",
    "        \"\"\"Team WAAAAAAEEEEEE Cassie\"\"\"\n",
    "        if date >= right_half_dates[Tindex]:\n",
    "            Tindex += 1\n",
    "        stormTval = Tvalues[Tindex]\n",
    "        \n",
    "        # if lightning exists, do dbscan on it\n",
    "        if len(subset != 0):\n",
    "            dbscan = DBSCAN(eps=.2, min_samples=3)\n",
    "            subset['Labels'] = dbscan.fit_predict(subset[['Long','Lat']])\n",
    "        \n",
    "        \"\"\"Team Squeem\"\"\"\n",
    "        # Setup the figure\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        \n",
    "        # this is for the coastline\n",
    "        # This reads the shapefile, extracts each shape, creates a polygon, and adds it to the list\n",
    "        sf = shapefile.Reader(shp_file)\n",
    "        shapes = sf.shapes()\n",
    "        Nshp = len(shapes)\n",
    "        ptchs = []\n",
    "        for nshp in range(Nshp):\n",
    "            pts = np.array(shapes[nshp].points)\n",
    "            prt = shapes[nshp].parts\n",
    "            par = list(prt) + [pts.shape[0]]\n",
    "            for pij in range(len(prt)):\n",
    "                ptchs.append(Polygon(pts[par[pij]:par[pij+1]]))\n",
    "        # Every every polygon from the shapefile to figure\n",
    "        ax.add_collection(PatchCollection(ptchs, facecolor= '#838688', edgecolor='k', linewidths=1., zorder=2))\n",
    "\n",
    "        # This adds the water color\n",
    "        ax.add_patch(mpl.patches.Rectangle((-180,-89),360,180,color='#a6cae0'))\n",
    "\n",
    "        # This connects Antarcitca to the bottom of the image\n",
    "        ax.add_patch(mpl.patches.Rectangle((-179.9,-89.9),360,2,color='#838688',zorder=3))\n",
    "\n",
    "        # These are more general image settings\n",
    "        plt.title(Storm_Names[dir_indx]+\" T\"+str(stormTval)+' '+str(date), fontdict = {'fontsize' : 40})\n",
    "\n",
    "        #edges = (-180,180,-90,90) # Left, Right, Bottom, Top | Set this if you want to change the map scale\n",
    "        edges = (center['Long'].values[0] - buffer, center['Long'].values[0] + buffer, center['Lat'].values[0] - buffer, center['Lat'].values[0] + buffer) # <--------------------------This affects map zoooooooooom\n",
    "\n",
    "        # setting up img stuff, prob shouldn't be touched\n",
    "        xlim = np.append(np.arange(edges[0], edges[1], step=2.5), edges[1])\n",
    "        ylim = np.append(np.arange(edges[2], edges[3], step=2.5), edges[3])\n",
    "        ax.set_xticks(xlim)\n",
    "        ax.set_yticks(ylim)\n",
    "        ax.set_xlim(edges[0], edges[1])\n",
    "        ax.set_ylim(edges[2], edges[3])\n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        plt.rc('xtick', labelsize=30)\n",
    "        plt.rc('ytick', labelsize=30)\n",
    "        \n",
    "        \"\"\"Team WAAAAAEEEEEE Cassie\"\"\"\n",
    "        # if lightning exists, put it in the image\n",
    "        if len(subset) != 0:\n",
    "            plt.scatter(subset['Long'], subset['Lat'], c=subset['Labels'], cmap=cmap, zorder=4) # plots the dbscan data\n",
    "        \n",
    "        \"\"\"Team Squeem\"\"\"\n",
    "        #plt.scatter(subset['Long'],subset['Lat'],s=100,c=\"yellow\",edgecolors='black',zorder=5) # <- old plot func for non-grouped data\n",
    "        plt.scatter(center['Long'], center['Lat'], s=140, c=\"red\", edgecolors='black' ,zorder=5, alpha=0.5) # plots the center of the storm\n",
    "\n",
    "        # save and close the image\n",
    "        plt.savefig(Image_Path[dir_indx] + str(index)+\".png\", bbox_inches='tight', pad_inches=.4)\n",
    "        if split_storm:\n",
    "            shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", T_Path[dir_indx] + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "            #plt.savefig(T_Path[dir_indx] + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index), bbox_inches='tight', pad_inches=.4)\n",
    "            shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", \"Images/T Value/\" + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "            #plt.savefig(\"Images/T Value/\" + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index), bbox_inches='tight', pad_inches=.4)\n",
    "        plt.close('all')\n",
    "\n",
    "        # Shows roughly how long we have left\n",
    "        taken = time.perf_counter() - starttime\n",
    "        print(f\"Storm #{dir_indx}: {Storm_Names[dir_indx]}, T{stormTval}\\tPercent Done: {(index + 1) / len(dates) * 100 : .2f}%\\tTime taken: {taken : .2f} seconds\\tEst time remaining: {datetime.timedelta(seconds=(len(dates) - (index + 1)) * taken)} {len(subset)}\", end='\\r')\n",
    "    dir_indx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAAEEEEE Cassie\"\"\"\n",
    "\"\"\"This makes the GIFs\"\"\"\n",
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    with imageio.get_writer(Image_Path[dir_indx]+'All.gif', mode='I') as writer:\n",
    "        Images = [f for f in listdir(Image_Path[dir_indx]) if isfile(join(Image_Path[dir_indx], f)) and f.endswith(\".png\")]\n",
    "        img_cnt = len(Images)\n",
    "        img_indx = 0\n",
    "        while img_indx < img_cnt:\n",
    "            image = imageio.imread(Image_Path[dir_indx] + str(img_indx) + \".png\")\n",
    "            writer.append_data(image)\n",
    "            img_indx+=1\n",
    "    dir_indx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b91f96a0a4f2baf9a7b9e7aa2a798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(description='Flash', options=(33237, 33310, 33315, 33318, 33337, 33369, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.do_plot(flash_id)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glmtools.io.glm import GLMDataset\n",
    "glm = GLMDataset('C:/Users/GamerPlayer1/Documents/Digipen/PersonalSVN/Spring22/CSP250/Storm_Data/GLM_setup/Dorian/240/18/OR_GLM-L2-LCFA_G16_s20192401800000_e20192401800200_c20192401800224.nc')\n",
    "flashes_subset = glm.subset_flashes(lon_range = (-74.55, -54.55), lat_range = (7.62, 27.62))\n",
    "\n",
    "from glmtools.plot.locations import plot_flash\n",
    "\n",
    "import ipywidgets as widgets\n",
    "# print(widgets.Widget.widget_types.values())\n",
    "fl_id_vals = list(flashes_subset.flash_id.data)\n",
    "fl_id_vals.sort()\n",
    "flash_slider =  widgets.SelectionSlider(\n",
    "    description='Flash',\n",
    "    options=fl_id_vals,\n",
    ")\n",
    "\n",
    "# from functools import partial\n",
    "# glm_plotter = partial(plot_flash, glm) # fails with a __name__ attr not found\n",
    "def do_plot(flash_id):\n",
    "    fig = plot_flash(glm, flash_id)\n",
    "widgets.interact(do_plot, flash_id=flash_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:asyncio:Using selector: SelectSelector\n",
      "DEBUG:asyncio:Using selector: SelectSelector\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:62444</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>24</li>\n",
       "  <li><b>Memory: </b>68.64 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:62444' processes=6 threads=24, memory=68.64 GB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notes:\n",
    "-use open_glm_time_series() in glmtools.io.imagery for timeframes >1 min --- saved as an xarray, not glm dataset\n",
    "-do data cleaning!!\n",
    "-from glmtools.io.glm import GLMDataset\n",
    "-[glm dataset].subset_flashes(lon_range = (..., ...), lat_range = (..., ...))\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "\n",
    "# This will automatically use multiple cores to perform the computation over datasets that don't fit in memory\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glmtools.io.imagery import gen_file_times\n",
    "import xarray as xr\n",
    "def open_glm_time_series_TEST(filenames, chunks=None):\n",
    "    \"\"\" Convenience function for combining individual 1-min GLM gridded imagery\n",
    "    files into a single xarray.Dataset with a time dimension.\n",
    "\n",
    "    Creates an index on the time dimension.\n",
    "\n",
    "    The time dimension will be in the order in which the files are listed\n",
    "    due to the behavior of combine='nested' in open_mfdataset.\n",
    "\n",
    "    Adjusts the time_coverage_start and time_coverage_end metadata.\n",
    "    \"\"\"\n",
    "    # Need to fix time_coverage_start and _end in concat dataset\n",
    "    starts = [t for t in gen_file_times(filenames)]\n",
    "    ends = [t for t in gen_file_times(filenames, time_attr='time_coverage_end')]\n",
    "\n",
    "    d = xr.open_mfdataset(filenames, concat_dim='time', chunks=chunks, combine='nested', drop_variables=('number_of_events'))\n",
    "    d['time'] = starts\n",
    "    d = d.set_index({'time':'time'})\n",
    "    d = d.set_coords('time')\n",
    "\n",
    "    d.attrs['time_coverage_start'] = pd.Timestamp(min(starts)).isoformat()\n",
    "    d.attrs['time_coverage_end'] = pd.Timestamp(max(ends)).isoformat()\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:arguments without labels along dimension 'number_of_events' cannot be aligned because they have different dimension sizes: {7680, 11776, 8194, 10248, 9736, 9233, 7699, 8725, 8730, 17950, 10271, 9760, 10272, 9764, 10788, 9766, 9773, 8238, 8243, 7222, 8759, 10294, 8762, 9276, 9789, 12861, 9279, 14916, 9796, 6726, 10822, 9284, 8265, 11849, 10315, 9287, 9297, 6756, 9326, 8305, 9843, 12406, 11382, 8840, 9353, 9876, 10390, 8348, 9372, 8863, 9378, 9379, 7333, 8869, 8871, 16551, 9900, 7341, 14510, 9397, 11958, 8377, 8890, 11451, 7359, 8895, 8897, 7362, 7363, 8907, 14540, 11983, 12513, 9965, 8430, 10992, 9457, 8947, 8949, 8442, 8956, 9210, 7939, 9475, 12550, 10503, 7433, 16138, 11531, 9996, 6413, 7952, 7953, 8465, 7457, 7458, 8994, 16166, 9510, 6954, 9011, 9524, 10035, 11059, 9016, 9018, 9023, 9536, 12097, 9537, 8006, 10058, 9037, 8014, 14671, 9040, 8534, 6486, 9047, 9558, 7516, 8545, 9575, 9577, 7018, 9067, 11627, 11631, 9587, 8057, 9090, 20358, 12167, 8072, 9607, 9100, 8590, 9618, 10645, 7062, 8597, 8092, 6046, 8094, 9122, 11170, 8613, 10153, 8106, 9131, 8622, 6575, 10159, 7089, 11698, 8633, 8634, 8636, 9661, 7625, 9164, 11216, 6098, 8675, 7143, 10215, 8172, 6640, 9714, 7667, 8183, 9720, 7674, 9726, 9215}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-85add4e6bbcd>\", line 14, in <module>\n",
      "    glm = open_glm_time_series_TEST(files)\n",
      "  File \"<ipython-input-12-30720fdbf3f0>\", line 18, in open_glm_time_series_TEST\n",
      "    d = xr.open_mfdataset(filenames, concat_dim='time', chunks=chunks, combine='nested', drop_variables=('number_of_events'))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\backends\\api.py\", line 935, in open_mfdataset\n",
      "    combine_attrs=combine_attrs,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\", line 351, in _nested_combine\n",
      "    combine_attrs=combine_attrs,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\", line 225, in _combine_nd\n",
      "    combine_attrs=combine_attrs,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\", line 254, in _combine_all_along_first_dim\n",
      "    datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\", line 284, in _combine_1d\n",
      "    combine_attrs=combine_attrs,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\concat.py\", line 239, in concat\n",
      "    objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\concat.py\", line 429, in _dataset_concat\n",
      "    align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\alignment.py\", line 348, in align\n",
      "    f\"arguments without labels along dimension {dim!r} cannot be \"\n",
      "ValueError: arguments without labels along dimension 'number_of_events' cannot be aligned because they have different dimension sizes: {7680, 11776, 8194, 10248, 9736, 9233, 7699, 8725, 8730, 17950, 10271, 9760, 10272, 9764, 10788, 9766, 9773, 8238, 8243, 7222, 8759, 10294, 8762, 9276, 9789, 12861, 9279, 14916, 9796, 6726, 10822, 9284, 8265, 11849, 10315, 9287, 9297, 6756, 9326, 8305, 9843, 12406, 11382, 8840, 9353, 9876, 10390, 8348, 9372, 8863, 9378, 9379, 7333, 8869, 8871, 16551, 9900, 7341, 14510, 9397, 11958, 8377, 8890, 11451, 7359, 8895, 8897, 7362, 7363, 8907, 14540, 11983, 12513, 9965, 8430, 10992, 9457, 8947, 8949, 8442, 8956, 9210, 7939, 9475, 12550, 10503, 7433, 16138, 11531, 9996, 6413, 7952, 7953, 8465, 7457, 7458, 8994, 16166, 9510, 6954, 9011, 9524, 10035, 11059, 9016, 9018, 9023, 9536, 12097, 9537, 8006, 10058, 9037, 8014, 14671, 9040, 8534, 6486, 9047, 9558, 7516, 8545, 9575, 9577, 7018, 9067, 11627, 11631, 9587, 8057, 9090, 20358, 12167, 8072, 9607, 9100, 8590, 9618, 10645, 7062, 8597, 8092, 6046, 8094, 9122, 11170, 8613, 10153, 8106, 9131, 8622, 6575, 10159, 7089, 11698, 8633, 8634, 8636, 9661, 7625, 9164, 11216, 6098, 8675, 7143, 10215, 8172, 6640, 9714, 7667, 8183, 9720, 7674, 9726, 9215}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arguments without labels along dimension 'number_of_events' cannot be aligned because they have different dimension sizes: {7680, 11776, 8194, 10248, 9736, 9233, 7699, 8725, 8730, 17950, 10271, 9760, 10272, 9764, 10788, 9766, 9773, 8238, 8243, 7222, 8759, 10294, 8762, 9276, 9789, 12861, 9279, 14916, 9796, 6726, 10822, 9284, 8265, 11849, 10315, 9287, 9297, 6756, 9326, 8305, 9843, 12406, 11382, 8840, 9353, 9876, 10390, 8348, 9372, 8863, 9378, 9379, 7333, 8869, 8871, 16551, 9900, 7341, 14510, 9397, 11958, 8377, 8890, 11451, 7359, 8895, 8897, 7362, 7363, 8907, 14540, 11983, 12513, 9965, 8430, 10992, 9457, 8947, 8949, 8442, 8956, 9210, 7939, 9475, 12550, 10503, 7433, 16138, 11531, 9996, 6413, 7952, 7953, 8465, 7457, 7458, 8994, 16166, 9510, 6954, 9011, 9524, 10035, 11059, 9016, 9018, 9023, 9536, 12097, 9537, 8006, 10058, 9037, 8014, 14671, 9040, 8534, 6486, 9047, 9558, 7516, 8545, 9575, 9577, 7018, 9067, 11627, 11631, 9587, 8057, 9090, 20358, 12167, 8072, 9607, 9100, 8590, 9618, 10645, 7062, 8597, 8092, 6046, 8094, 9122, 11170, 8613, 10153, 8106, 9131, 8622, 6575, 10159, 7089, 11698, 8633, 8634, 8636, 9661, 7625, 9164, 11216, 6098, 8675, 7143, 10215, 8172, 6640, 9714, 7667, 8183, 9720, 7674, 9726, 9215}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-85add4e6bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-85add4e6bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0maccum_mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mglm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_glm_time_series_TEST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-30720fdbf3f0>\u001b[0m in \u001b[0;36mopen_glm_time_series_TEST\u001b[1;34m(filenames, chunks)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mends\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_file_times\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_attr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_coverage_end'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nested'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number_of_events'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstarts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\backends\\api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[1;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                 \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m             )\n\u001b[0;32m    937\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcombine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"by_coords\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_nested_combine\u001b[1;34m(datasets, concat_dims, compat, data_vars, coords, ids, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_nd\u001b[1;34m(combined_ids, concat_dims, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         )\n\u001b[0;32m    227\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mcombined_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_all_along_first_dim\u001b[1;34m(combined_ids, dim, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         new_combined_ids[new_id] = _combine_1d(\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_combined_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_1d\u001b[1;34m(datasets, concat_dim, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                 \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombine_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m             )\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m     return f(\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36m_dataset_concat\u001b[1;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     datasets = list(\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[0malign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\glmval\\lib\\site-packages\\xarray\\core\\alignment.py\u001b[0m in \u001b[0;36malign\u001b[1;34m(join, copy, indexes, exclude, fill_value, *objects)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_indexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             raise ValueError(\n\u001b[1;32m--> 348\u001b[1;33m                 \u001b[1;34mf\"arguments without labels along dimension {dim!r} cannot be \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[1;34mf\"aligned because they have different dimension sizes: {sizes!r}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: arguments without labels along dimension 'number_of_events' cannot be aligned because they have different dimension sizes: {7680, 11776, 8194, 10248, 9736, 9233, 7699, 8725, 8730, 17950, 10271, 9760, 10272, 9764, 10788, 9766, 9773, 8238, 8243, 7222, 8759, 10294, 8762, 9276, 9789, 12861, 9279, 14916, 9796, 6726, 10822, 9284, 8265, 11849, 10315, 9287, 9297, 6756, 9326, 8305, 9843, 12406, 11382, 8840, 9353, 9876, 10390, 8348, 9372, 8863, 9378, 9379, 7333, 8869, 8871, 16551, 9900, 7341, 14510, 9397, 11958, 8377, 8890, 11451, 7359, 8895, 8897, 7362, 7363, 8907, 14540, 11983, 12513, 9965, 8430, 10992, 9457, 8947, 8949, 8442, 8956, 9210, 7939, 9475, 12550, 10503, 7433, 16138, 11531, 9996, 6413, 7952, 7953, 8465, 7457, 7458, 8994, 16166, 9510, 6954, 9011, 9524, 10035, 11059, 9016, 9018, 9023, 9536, 12097, 9537, 8006, 10058, 9037, 8014, 14671, 9040, 8534, 6486, 9047, 9558, 7516, 8545, 9575, 9577, 7018, 9067, 11627, 11631, 9587, 8057, 9090, 20358, 12167, 8072, 9607, 9100, 8590, 9618, 10645, 7062, 8597, 8092, 6046, 8094, 9122, 11170, 8613, 10153, 8106, 9131, 8622, 6575, 10159, 7089, 11698, 8633, 8634, 8636, 9661, 7625, 9164, 11216, 6098, 8675, 7143, 10215, 8172, 6640, 9714, 7667, 8183, 9720, 7674, 9726, 9215}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glmtools.io.imagery import open_glm_time_series, aggregate\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "files = [os.path.join('C:/Users/GamerPlayer1/Documents/Digipen/PersonalSVN/Spring22/CSP250/Storm_Data/GLM_setup/Dorian/240/18/', fn) \n",
    "        for fn in os.listdir('C:/Users/GamerPlayer1/Documents/Digipen/PersonalSVN/Spring22/CSP250/Storm_Data/GLM_setup/Dorian/240/18/')\n",
    "    ]\n",
    "\n",
    "accum_mins = 60\n",
    "try:\n",
    "    glm = open_glm_time_series_TEST(files)\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n",
    "print(glm)\n",
    "\n",
    "# accumulate over 3 minutes. To also subset a long dataset, see the start_end kwarg\n",
    "agglm = aggregate(glm, accum_mins)\n",
    "# Set the time coordinate to the start time of each time bin (could also choose mid)\n",
    "agglm['time_bins'] = [v.left for v in agglm.time_bins.values]\n",
    "glm_grids = agglm.rename({'time_bins':'time'})\n",
    "print(glm_grids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from glmtools.plot.grid import plot_glm_grid\n",
    "\n",
    "fields_6panel = ['flash_extent_density', 'minimum_flash_area','total_energy', \n",
    "                 'group_extent_density', 'average_group_area', 'group_centroid_density']\n",
    "\n",
    "\n",
    "def plot(w, fig=None, time_widget=None, field_widget=None, subplots=(2,3), fields=None):\n",
    "    t = pd.to_datetime(time_widget.value)\n",
    "    n_subplots = subplots[0] * subplots[1]\n",
    "    if fields is None:\n",
    "        if n_subplots == 1:\n",
    "            fields = [field_widget.value]\n",
    "        else:\n",
    "            fields = fields_6panel[0:n_subplots]\n",
    "\n",
    "    mapax, cbar_obj = plot_glm_grid(fig, glm_grids, t, fields, subplots=subplots,\n",
    "                                   axes_facecolor = (1.0, 1.0, 1.0), map_color = (0.2, 0.2, 0.2))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
