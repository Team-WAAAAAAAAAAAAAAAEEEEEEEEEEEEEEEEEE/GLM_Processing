{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # makes the notebook fill the whole window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "#import matplotlib.patches as patches\n",
    "from matplotlib import ticker\n",
    "from matplotlib.collections import PatchCollection\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Polygon\n",
    "from sklearn.cluster import DBSCAN\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import shapefile\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import CubicSplineTrack\n",
    "#(c)2015 Robert Sedgewick, Kevin Wayne, and Robert Dondero\n",
    "from greatcircle import great_circle\n",
    "from datetime import date\n",
    "import shutil\n",
    "import imageio\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Turn off interactive plotting for pyplot\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"\"\"\n",
    "This cell is for config variables ONLY\n",
    "\"\"\"\n",
    "check_minutes = 60 # minuite var\n",
    "split_storm = 1 # seperate or no\n",
    "is_wwlln = 0 # wwlln(1) or glm(0)\n",
    "initial_data_path = \"Data/\"\n",
    "do_all_storms = 0 # 1 for all storms, 0 for specific storms. uses storms_to_run if 0\n",
    "storms_to_run = [\"Arthur\", \"Bertha\", \"Beta\", \"Delta\", \"Dolly\", \"Edouard\"]\n",
    "\n",
    "# ncdf to csv config\n",
    "glm_ncdf_data_path = \"C:/Users/lance/Documents/CSP250/GLM/Storms\"\n",
    "glm_csv_minute_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Conversion For Arthur\n",
      "Start Conversion For Bertha\n",
      "File at \"C:/Users/lance/Documents/CSP250/GLM/Storms\\Bertha/149\\07\\OR_GLM-L2-LCFA_G16_s20201490717200_e20201490717200_c20201490718138.nc\" has exception:\n",
      "'NoneType' object has no attribute 'groups'\n",
      "When being saved to \"Data/Bertha//glm/149\\07/0.0_60.0.csv\"\n"
     ]
    }
   ],
   "source": [
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"\"\"\n",
    "Cassie\n",
    "Converts multiple .nc files to .csv files\n",
    "For each input file, read it using GLMDataset\n",
    "then save only the values we want\n",
    "\"\"\"\n",
    "import csv\n",
    "from glmtools.io.glm import GLMDataset\n",
    "\n",
    "def convert_ncdf_to_csv(file_paths, out_path, center):\n",
    "    with open(out_path, 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow([\"Date\", \"Lat\", \"Long\"])\n",
    "\n",
    "        for in_file_path in file_paths:\n",
    "            #print(f\"Path: {in_file_path}\")\n",
    "            \n",
    "            try:\n",
    "                glm_data = GLMDataset(in_file_path).dataset\n",
    "            except Exception as excpt:\n",
    "                print(f\"File at \\\"{in_file_path}\\\" has exception:\")\n",
    "                print(excpt)\n",
    "                print(f\"When being saved to \\\"{out_path}\\\"\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            groups = glm_data[['group_energy','group_area']]\n",
    "            group_time = groups['group_time_offset'].values\n",
    "            group_lon = groups['group_lon'].values\n",
    "            group_lat = groups['group_lat'].values\n",
    "\n",
    "            group_size = (glm_data['number_of_groups'].values).size\n",
    "            group_indx = 0\n",
    "            while group_indx < group_size:\n",
    "                #print(f\"Lat Center: {center['Lat']}, Long Center: {center['Long']}\")\n",
    "                #print(f\"Lat Group: {group_lat[group_indx]}, Long Group: {group_lon[group_indx]}\")\n",
    "                distance_to_center = great_circle(group_lat[group_indx], group_lon[group_indx], (center['Lat']).iloc[0], (center['Long']).iloc[0])\n",
    "                #print(f\"Distance: {distance_to_center}\")\n",
    "\n",
    "                if ((distance_to_center <= 100 / 1.852) or ((distance_to_center >= 200 / 1.852) and (distance_to_center <= 400 / 1.852))):\n",
    "                    writer.writerow([group_time[group_indx], group_lat[group_indx], group_lon[group_indx]])\n",
    "\n",
    "                group_indx+=1\n",
    "    return\n",
    "\n",
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"\"\"\n",
    "Cassie\n",
    "Testing code for converting local .nc files to .csv files\n",
    "Gets all files for ONLY one hour, devided into chunks\n",
    "YOU MUST CHANGE THE PATHS TO GET THIS TO WORK\n",
    "AND MAKE SURE ALL FOLDERS HAVE BEEN CREATED BEFOREHAND\n",
    "\"\"\"\n",
    "\n",
    "from os import listdir\n",
    "def convert_storms():\n",
    "    global storms_to_run\n",
    "    if do_all_storms:\n",
    "        storms_to_run = listdir(glm_ncdf_data_path)\n",
    "    ncdf_storm_paths = [os.path.join(glm_ncdf_data_path, fn+'/') for fn in storms_to_run]\n",
    "    csv_storm_paths = [os.path.join(initial_data_path, fn+'/') for fn in storms_to_run]\n",
    "    spline_paths = [\"\"] * len(storms_to_run)\n",
    "\n",
    "    curr_storm_indx = 0\n",
    "    while curr_storm_indx < len(csv_storm_paths):\n",
    "        if not os.path.exists(csv_storm_paths[curr_storm_indx]):\n",
    "            os.mkdir(csv_storm_paths[curr_storm_indx])\n",
    "        \n",
    "        files = listdir(csv_storm_paths[curr_storm_indx])\n",
    "        file_indx = 0\n",
    "        spline_found = False\n",
    "        while file_indx < len(files):\n",
    "            if files[file_indx].endswith(\"Trackfile.csv\"):\n",
    "                spline_paths[curr_storm_indx] = join(csv_storm_paths[curr_storm_indx]+\"/\", files[file_indx])\n",
    "                spline_found = True\n",
    "                break\n",
    "            file_indx+=1\n",
    "\n",
    "        if not spline_found:\n",
    "            print(\"Spline Not Found For Storm \" + storms_to_run[curr_storm_indx])\n",
    "            return\n",
    "        \n",
    "        csv_storm_paths[curr_storm_indx] = os.path.join(csv_storm_paths[curr_storm_indx]+'/', \"glm/\")\n",
    "\n",
    "        if not os.path.exists(csv_storm_paths[curr_storm_indx]):\n",
    "            os.mkdir(csv_storm_paths[curr_storm_indx])\n",
    "        \n",
    "        curr_storm_indx+=1\n",
    "    \n",
    "\n",
    "    curr_storm_indx = 0\n",
    "    while curr_storm_indx < len(ncdf_storm_paths):\n",
    "        print(f\"Start Conversion For {storms_to_run[curr_storm_indx]}\")\n",
    "        ncdf_day_paths = [os.path.join(ncdf_storm_paths[curr_storm_indx], fn) for fn in listdir(ncdf_storm_paths[curr_storm_indx])]\n",
    "        csv_day_paths = [os.path.join(csv_storm_paths[curr_storm_indx], fn) for fn in listdir(ncdf_storm_paths[curr_storm_indx])]\n",
    "        storm_center = pd.read_csv(Cubic_Spline_Path[Storm_Names.index(\"Hurricane \" + storms_to_run[curr_storm_indx])])\n",
    "        #print(Cubic_Spline_Path[Storm_Names.index(\"Hurricane \" + (storms_to_run[curr_storm_indx])[0:-1])])\n",
    "        #print(storms_to_run[curr_storm_indx])\n",
    "        storm_center['Date'] = storm_center['Date'].apply(pd.to_datetime)\n",
    "        start_date = storm_center['Date'][0]\n",
    "        end_date = storm_center['Date'][len(storm_center)-1]\n",
    "        ncdf_time_offsets = [start_date.timetuple().tm_yday - int(ncdf_day_paths[0][-3:]), start_date.hour]\n",
    "        start_date = datetime.datetime(start_date.year, start_date.month, start_date.day)\n",
    "        #print(ncdf_time_offsets)\n",
    "        #print(center_date)\n",
    "        #print(storm_center)\n",
    "        breakout_flag = False\n",
    "        \n",
    "\n",
    "        curr_day_indx = ncdf_time_offsets[0]\n",
    "        while curr_day_indx < len(ncdf_day_paths) and not breakout_flag:\n",
    "            ncdf_hour_paths = [os.path.join(ncdf_day_paths[curr_day_indx], fn) for fn in listdir(ncdf_day_paths[curr_day_indx])]\n",
    "            csv_hour_paths = [os.path.join(csv_day_paths[curr_day_indx], fn) for fn in listdir(ncdf_day_paths[curr_day_indx])]\n",
    "            \n",
    "\n",
    "            if not os.path.exists(csv_day_paths[curr_day_indx]):\n",
    "                os.mkdir(csv_day_paths[curr_day_indx])\n",
    "            \n",
    "\n",
    "            if curr_day_indx == ncdf_time_offsets[0]:\n",
    "                curr_hour_indx = ncdf_time_offsets[1]\n",
    "            else:\n",
    "                curr_hour_indx = 0\n",
    "            \n",
    "            while curr_hour_indx < len(ncdf_hour_paths) and not breakout_flag:\n",
    "                ncdf_file_paths = [os.path.join(ncdf_hour_paths[curr_hour_indx], fn) for fn in listdir(ncdf_hour_paths[curr_hour_indx])]\n",
    "                \n",
    "                if not os.path.exists(csv_hour_paths[curr_hour_indx]):\n",
    "                    os.mkdir(csv_hour_paths[curr_hour_indx])\n",
    "\n",
    "                files_per_minuite = 3\n",
    "                offset = glm_csv_minute_length*files_per_minuite\n",
    "\n",
    "                center_date = start_date + datetime.timedelta(curr_day_indx, curr_hour_indx*60*60)\n",
    "                center = storm_center[storm_center['Date'] == center_date]\n",
    "\n",
    "                #convert_ncdf_to_csv(ncdf_file_paths, os.path.join(csv_hour_paths[curr_hour_indx]+'/', f'0_60.csv'), center)\n",
    "                #print(center_date)\n",
    "                #print(f\"Date: {center_date}, Lat Center: {center['Lat']}, Long Center: {center['Long']}\")\n",
    "                indx = 0\n",
    "                while indx < 60*files_per_minuite and not breakout_flag:\n",
    "                    convert_ncdf_to_csv(ncdf_file_paths[indx:indx+offset], os.path.join(csv_hour_paths[curr_hour_indx]+'/', f'{indx/files_per_minuite}_{(indx+offset)/files_per_minuite}.csv'), center)\n",
    "                    if end_date == center_date:\n",
    "                        breakout_flag = True\n",
    "                        break\n",
    "                \n",
    "                    indx += glm_csv_minute_length*files_per_minuite\n",
    "                    if (indx % glm_csv_minute_length == 0):\n",
    "                        center_date = start_date + datetime.timedelta(curr_day_indx, curr_hour_indx*60*60 + (indx/files_per_minuite)*60)\n",
    "                        center = storm_center[storm_center['Date'] == center_date]\n",
    "                        #print(f\"Date: {center_date}, Lat Center: {center['Lat']}, Long Center: {center['Long']}\")\n",
    "                curr_hour_indx += 1\n",
    "            curr_day_indx += 1\n",
    "        curr_storm_indx += 1\n",
    "\n",
    "convert_storms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data/Arthur', 'Bertha', 'Beta', 'Delta', 'Dolly', 'Edouard']\n",
      "['Data/Arthur', 'Data/Bertha', 'Beta', 'Delta', 'Dolly', 'Edouard']\n",
      "['Data/Arthur', 'Data/Bertha', 'Data/Beta', 'Delta', 'Dolly', 'Edouard']\n",
      "['Data/Arthur', 'Data/Bertha', 'Data/Beta', 'Data/Delta', 'Dolly', 'Edouard']\n",
      "['Data/Arthur', 'Data/Bertha', 'Data/Beta', 'Data/Delta', 'Data/Dolly', 'Edouard']\n",
      "['Data/Arthur', 'Data/Bertha', 'Data/Beta', 'Data/Delta', 'Data/Dolly', 'Data/Edouard']\n",
      "Storms Found: 6\n",
      "['Hurricane Arthur', 'Hurricane Bertha', 'Hurricane Beta', 'Hurricane Delta', 'Hurricane Dolly', 'Hurricane Edouard']\n",
      "['Data/Arthur/glm', 'Data/Bertha/glm', 'Data/Beta/glm', 'Data/Delta/glm', 'Data/Dolly/glm', 'Data/Edouard/glm']\n"
     ]
    }
   ],
   "source": [
    "#-----------------------#\n",
    "\" UPDATED FOR MILESTONE \"\n",
    "#-----------------------#\n",
    "\"\"\"Team WAAAAAAEEEEE Cassie\"\"\"\n",
    "\n",
    "if do_all_storms:\n",
    "    only_dirs = [d for d in listdir(initial_data_path) if isdir(join(initial_data_path, d))] # makes a list of all the folders in Data/\n",
    "else:\n",
    "    only_dirs = [d for d in storms_to_run]\n",
    "\n",
    "dir_count = len(only_dirs)\n",
    "\n",
    "WWLLN_Track_Path = [\"\"] * dir_count # the paths of all the track files, in the same order as only_dirs\n",
    "WWLLN_Locations_Path = [\"\"] * dir_count # the paths of all the lightning locations files, in the same order as only_dirs\n",
    "Cubic_Spline_Path = [\"\"] * dir_count # the paths of all the cubic spline files, in the same order as only_dirs\n",
    "glm_paths = [\"\"] * dir_count\n",
    "\n",
    "Image_Path = [\"\"] * dir_count # Base Image paths\n",
    "Storm_Names = [\"\"] * dir_count # Base storm names\n",
    "T_Path = [\"\"] * dir_count\n",
    "\n",
    "# loop thru all dirs\n",
    "storms_found = 0\n",
    "def load_file_paths():\n",
    "    global storms_found\n",
    "    dir_indx = 0\n",
    "    while dir_indx < dir_count:\n",
    "        \n",
    "        # this gets all the file paths in a given storms' folder\n",
    "        only_dirs[dir_indx] = join(initial_data_path, only_dirs[dir_indx])\n",
    "        print(only_dirs)\n",
    "\n",
    "        glm_paths[dir_indx] = only_dirs[dir_indx] + '/glm' #Lance code start here\n",
    "        if not os.path.exists(glm_paths[dir_indx]):\n",
    "            glm_paths[dir_indx] = '' #Lance code end here\n",
    "        \n",
    "        file_paths = listdir(only_dirs[dir_indx])\n",
    "        \n",
    "        # check if the 3 nessisary files exist\n",
    "        if len(file_paths) < 2:\n",
    "            only_dirs[dir_indx] = \"\"\n",
    "            dir_indx+=1\n",
    "            continue\n",
    "        \n",
    "        #print(only_dirs[dir_indx])\n",
    "        #print(file_paths)\n",
    "        \n",
    "        # for every file in file_paths, sort it into its' respective arrays\n",
    "        file_indx = 0\n",
    "        files_found = 0\n",
    "        Cubic_Found = False\n",
    "        while file_indx < len(file_paths):\n",
    "            # creates the files full path\n",
    "            file_paths[file_indx] = join(only_dirs[dir_indx]+\"/\", file_paths[file_indx])\n",
    "            #print(file_paths[file_indx])\n",
    "            \n",
    "            # sorts the file path based on the last expected word of a file and it's type\n",
    "            if file_paths[file_indx].endswith(\"Trackfile.txt\"):\n",
    "                WWLLN_Track_Path[dir_indx] = file_paths[file_indx]\n",
    "                files_found+=1\n",
    "                \n",
    "                # generates the name of the storm for images\n",
    "                Storm_Names[dir_indx] = \"Hurricane \" + WWLLN_Track_Path[dir_indx].rsplit(\"_\", 3)[1]\n",
    "                \n",
    "            elif file_paths[file_indx].endswith(\"Locations.txt\"):\n",
    "                WWLLN_Locations_Path[dir_indx] = file_paths[file_indx]\n",
    "                files_found+=1\n",
    "                \n",
    "            elif file_paths[file_indx].endswith(\"Trackfile.csv\"):\n",
    "                Cubic_Spline_Path[dir_indx] = file_paths[file_indx]\n",
    "                Cubic_Found = True\n",
    "                files_found+=1\n",
    "            #print(f\"File: {file_paths[file_indx]}, Storm indx: {dir_indx}\\n\")\n",
    "            file_indx+=1\n",
    "        \n",
    "        # Run with Bens' Cubic Spline\n",
    "        if not Cubic_Found:\n",
    "            Cubic_Spline_Path[dir_indx] = join(only_dirs[dir_indx]+\"/\", Storm_Names[dir_indx] + \" Cubic Spline Trackfile.csv\")\n",
    "            #print(WWLLN_Track_Path[dir_indx])\n",
    "            CubicSplineTrack.cubic_spline_trackfile(WWLLN_Track_Path[dir_indx], Cubic_Spline_Path[dir_indx], check_minutes)\n",
    "            files_found+=1\n",
    "        \n",
    "        if files_found != 3:\n",
    "            print(f\"Incorrect files found: {files_found}, {file_paths}\\n\")\n",
    "            only_dirs[dir_indx] = \"\"\n",
    "            dir_indx+=1\n",
    "            continue\n",
    "        \n",
    "        # makes the path for this index's storm\n",
    "        if is_wwlln:\n",
    "            Image_Path[dir_indx] = join(\"./Images/\", Storm_Names[dir_indx])\n",
    "        else:\n",
    "            Image_Path[dir_indx] = \"./Images/glm_\" + Storm_Names[dir_indx]\n",
    "        \n",
    "        if not os.path.exists(Image_Path[dir_indx]):\n",
    "            os.mkdir(Image_Path[dir_indx])\n",
    "        \n",
    "        T_Path[dir_indx] = Image_Path[dir_indx] + \"/T Value\"\n",
    "        Image_Path[dir_indx] = Image_Path[dir_indx] + \"/\" + Storm_Names[dir_indx]\n",
    "        \n",
    "        if split_storm:\n",
    "            Image_Path[dir_indx] = Image_Path[dir_indx] + \" Split \" + str(check_minutes) +\" Minutes\"\n",
    "        else:\n",
    "            Image_Path[dir_indx] = Image_Path[dir_indx] + \" \" + str(check_minutes) +\" Minutes\"\n",
    "        \n",
    "        # if an image folder |dosen't| exist, create a new one with the path at Image_Path[dir_indx]\n",
    "        if not os.path.exists(Image_Path[dir_indx]):\n",
    "            os.mkdir(Image_Path[dir_indx])\n",
    "        \n",
    "        Image_Path[dir_indx] = Image_Path[dir_indx] + '/' # add a slash for use in the next block, making the actual img paths\n",
    "        \n",
    "        if not os.path.exists(T_Path[dir_indx]):\n",
    "            os.mkdir(T_Path[dir_indx])\n",
    "            T_Path[dir_indx] = T_Path[dir_indx] + '/'\n",
    "            if not os.path.exists(T_Path[dir_indx]+\"0.0\"):\n",
    "                os.mkdir(T_Path[dir_indx]+\"0.0\")\n",
    "            T_value = 1.5\n",
    "            while T_value < 9:\n",
    "                if not os.path.exists(T_Path[dir_indx]+str(T_value)):\n",
    "                    os.mkdir(T_Path[dir_indx]+str(T_value))\n",
    "                T_value+=0.5\n",
    "        else:\n",
    "            T_Path[dir_indx] = T_Path[dir_indx] + '/'\n",
    "        \n",
    "        #print('\\n')\n",
    "        storms_found+=1\n",
    "        dir_indx+=1\n",
    "\n",
    "    if not os.path.exists(\"Images/T Value\"):\n",
    "        os.mkdir(\"Images/T Value\")\n",
    "        if not os.path.exists(\"Images/T Value/0.0\"):\n",
    "            os.mkdir(\"Images/T Value/0.0\")\n",
    "        T_value = 1.5\n",
    "        while T_value < 9:\n",
    "            if not os.path.exists(\"Images/T Value/\"+str(T_value)):\n",
    "                os.mkdir(\"Images/T Value/\"+str(T_value))\n",
    "            T_value+=0.5\n",
    "\n",
    "load_file_paths()\n",
    "# print for debug\n",
    "print(f\"Storms Found: {storms_found}\")\n",
    "print(Storm_Names)\n",
    "print(glm_paths)\n",
    "#print(Image_Path)\n",
    "#print(WWLLN_Track_Path)\n",
    "#print(WWLLN_Locations_Path)\n",
    "#print(Cubic_Spline_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAEEEEEEEEE Lance\"\"\"\n",
    "def run_circle(row, center_lat, center_lon): \n",
    "    lat = (row['Lat'])\n",
    "    lon = (row['Long'])\n",
    "    return great_circle(lat, lon, center_lat, center_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAAEEEEEEEEEE Bhargav\"\"\"\n",
    "#Creating intensity table\n",
    "intensity_values = {\n",
    "     'T-Number': [1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5],\n",
    "  '1-min Winds': [25, 30, 35, 45, 55, 65, 77, 90, 102, 115, 127, 140, 155, 170, 185],\n",
    "'Min. Pressure': [0, 1009, 1005, 1000, 994, 987, 979, 970, 960, 948, 935, 921, 906, 890, 873]\n",
    "}\n",
    "T_frame = pd.DataFrame(intensity_values)\n",
    "\n",
    "def Tvalue(wind_speed):\n",
    "    i=0\n",
    "    half=0\n",
    "    current = 0\n",
    "    prev=0\n",
    "    after=0\n",
    "    for i in range(15):\n",
    "        current = T_frame.loc[i, '1-min Winds']\n",
    "        if (i == 0):\n",
    "            prev=20\n",
    "        else:\n",
    "            prev = T_frame.loc[(i-1), '1-min Winds']\n",
    "        \n",
    "        if(i==14):\n",
    "            after = 188\n",
    "            \n",
    "        else:\n",
    "            after=T_frame.loc[(i+1), '1-min Winds']\n",
    "\n",
    "        #if(wind_speed < 25):\n",
    "            #return 1.5\n",
    "        \n",
    "        if(wind_speed > 185):\n",
    "            return 8.5\n",
    "        \n",
    "        else:\n",
    "            half_prev =current - ((current-prev)/2)\n",
    "            half_aft = current + ((after-prev)/2)\n",
    "            if(wind_speed >= half_prev and wind_speed < half_aft):\n",
    "                return T_frame.loc[i, 'T-Number']\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"Lance\" \n",
    "def get_glm_data_path(day, hour, start_minute, base_path):\n",
    "    temp_path = base_path + '/' + str(day)\n",
    "    #print(temp_path)\n",
    "\n",
    "    start_string = \"0.0\"\n",
    "    hour_string = \"\"\n",
    "    #end_string = \"__.csv\"\n",
    "    if hour < 10:\n",
    "        hour_string = f\"0{hour}\"\n",
    "    else:\n",
    "        hour_string = f\"{hour}\"\n",
    "    temp_path += '/' + hour_string\n",
    "    \n",
    "    files_in_path = listdir(temp_path)\n",
    "    #print(files_in_path)\n",
    "    file_found = False\n",
    "    for file in files_in_path:\n",
    "        if file.startswith(start_string):\n",
    "            temp_path = join(temp_path +'/', file)\n",
    "            file_found = True\n",
    "    \n",
    "    if not file_found:\n",
    "        return \"\"\n",
    "    \n",
    "    return temp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"Bhargava's Code\"\n",
    "#Function to get and arrange generated csv file\n",
    "def get_csv(csv_file_path):\n",
    "    temp_csv = pd.read_csv(csv_file_path)\n",
    "    temp_csv[\"Date\"] = temp_csv[\"Date\"].apply(pd.to_datetime)\n",
    "    temp_csv= temp_csv.sort_values([\"Date\"], ascending=True)\n",
    "    if len(temp_csv[\"Date\"]) == 0:\n",
    "        return temp_csv\n",
    "    index = pd.Series(range(int(temp_csv.index.max())+1))\n",
    "    temp_csv=temp_csv.set_index(index)\n",
    "    return temp_csv\n",
    "\n",
    "#------------------------#\n",
    "\" NEW CODE FOR MILESTONE \"\n",
    "#------------------------#\n",
    "\"Cassie\"\n",
    "def load_glm_time_range(date_val, storm_index):\n",
    "    #print(date_val)\n",
    "    day = date_val.timetuple().tm_yday\n",
    "    #print(get_glm_data_path(day, date_val.hour, date_val.minute, glm_paths[storm_index]))\n",
    "    data_out = [get_csv(get_glm_data_path(day, date_val.hour, date_val.minute, glm_paths[storm_index]))]\n",
    "    indx = glm_csv_minute_length\n",
    "\n",
    "    if glm_csv_minute_length >= check_minutes:\n",
    "        return data_out[0]\n",
    "\n",
    "    new_date = date_val\n",
    "    while indx < check_minutes:\n",
    "        new_date = new_date + datetime.timedelta(minutes=check_minutes)\n",
    "        day = (new_date - datetime.datetime(new_date.year, 1, 1)).days\n",
    "        data_out.append(get_csv(get_glm_data_path(day, new_date.hour, new_date.minute, glm_paths[storm_index])))\n",
    "        indx += glm_csv_minute_length\n",
    "    \n",
    "    return pd.concat(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm #5: Hurricane Edouard, T2.5\tPercent Done:  100.00%\tTime taken:  0.87 seconds\tEst time remaining: 0:00:00 08315 157207 112628\r"
     ]
    }
   ],
   "source": [
    "#-----------------------#\n",
    "\" UPDATED FOR MILESTONE \"\n",
    "#-----------------------#\n",
    "\"\"\"\n",
    "TODO:\n",
    "-Clean up code for easier and more clear running & flow\n",
    "-statitics over a single storm and multiple storms\n",
    "-Test DBSCAN settings\n",
    "\"\"\"\n",
    "# for every storm, do the things\n",
    "\"\"\"Team WAAAAAAAEEEEEE Cassie\"\"\"\n",
    "def main():\n",
    "    disable_ploting = 0\n",
    "    dir_indx = 0\n",
    "    while dir_indx < dir_count:\n",
    "        # if storm is invalid, skip doin the things\n",
    "        if only_dirs[dir_indx] == \"\":\n",
    "            dir_indx+=1\n",
    "            continue\n",
    "        \n",
    "        \"\"\"Team Squeem\"\"\"\n",
    "        # This loads the WWLLN trackfile Jeremy and Natalia give us\n",
    "        # wind = nautical mph, pressure = millibars\n",
    "        df = pd.read_csv(WWLLN_Track_Path[dir_indx], header=None, names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Lat\",\"Long\",\"Min_Pressure\",\"Max_Winds\",\"Unused\"], low_memory=False, sep='\\t')\n",
    "        df = df.drop(\"Unused\", axis=1)#data filtering\n",
    "        df['Date'] = df.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}\", format=\"%Y-%m-%d-%H\"), axis=1)\n",
    "        \n",
    "        \n",
    "        \"Team WAAAAAEEEEE Lance\"\n",
    "        #wind pressure stuff \n",
    "        #run pressure wind speed function on df for every row, save into array\n",
    "        Tindex = 0\n",
    "        Tvalues = df['Max_Winds'].apply(Tvalue)\n",
    "        #Tvalues = [Tvalue(t) for t in df['Max_Winds'] if isdir(join(\"Data/\", d))]\n",
    "        #make dataframe \n",
    "        dates = df['Date']\n",
    "        #print(Tvalues)\n",
    "        #print(df['Max_Winds'])\n",
    "        if len(Tvalues) != len(dates):\n",
    "            raise Exception(\"Lengths differ!\")\n",
    "        #loop through df, for every row, check date \n",
    "        df_length = len(dates)\n",
    "        left_half_dates = [date.min] * df_length\n",
    "        right_half_dates = [date.min] * df_length\n",
    "        date_index = 1 \n",
    "        left_half_dates[0] = date.min\n",
    "        right_half_dates[df_length - 1] = date.max\n",
    "        while date_index < df_length: \n",
    "            left_half_dates[date_index] = dates.iloc[date_index - 1] + ((dates.iloc[date_index] - dates.iloc[date_index - 1]) / 2)\n",
    "            date_index += 1\n",
    "        date_index = 0\n",
    "        while date_index < df_length - 1: \n",
    "            right_half_dates[date_index] = dates.iloc[date_index] + ((dates.iloc[date_index] - dates.iloc[date_index + 1]) / 2)\n",
    "            date_index += 1\n",
    "        \n",
    "        \"\"\"Team Squeem\"\"\"\n",
    "        if is_wwlln:\n",
    "            # This loads the WWLLN lighting location data that Jeremy and Natalia give us\n",
    "            ln = pd.read_csv(WWLLN_Locations_Path[dir_indx], header=None, names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Min\",\"Sec\",\"Lat\",\"Long\",\"Dist_East_West\",\"Dist_North_South\"], low_memory=False, sep=' ')\n",
    "            ln['Date'] = ln.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}-{int(x['Min'])}-{x['Sec']}\", format=\"%Y-%m-%d-%H-%M-%S.%f\"), axis=1)#is a data clean\n",
    "        else:\n",
    "            ln = 0\n",
    "            # other GLM processing\n",
    "\n",
    "        # This opens the cubic spline trackfiles that Ben creates\n",
    "        storm_center = pd.read_csv(Cubic_Spline_Path[dir_indx])\n",
    "        storm_center['Date'] = storm_center['Date'].apply(pd.to_datetime)\n",
    "\n",
    "        # Genereates the list of dates from the WWLLN trackfile of a (check_minutes) range\n",
    "        dates = pd.date_range(df['Date'][0], df['Date'][len(df)-1], freq=str(check_minutes)+\"T\")\n",
    "        \n",
    "        buffer = 10\n",
    "        if split_storm:\n",
    "            buffer = 6 # affects map zooooooooooooom\n",
    "        shp_file = \"World_Countries__Generalized_-shp/World_Countries__Generalized_\" # Map file ???????\n",
    "        cmap = 'gist_ncar' # dunno WHAT this is\n",
    "\n",
    "        # Counter and iteration\n",
    "        for index, date_val in enumerate(dates): # this loop makes one image for every (check_minutes) amount of time\n",
    "            starttime = time.perf_counter() # speeed timer start\n",
    "            center = storm_center[storm_center['Date'] == date_val] # storm center at this time frame\n",
    "            \n",
    "            if not is_wwlln:\n",
    "                ln = load_glm_time_range(date_val, dir_indx)\n",
    "                #if len(ln):\n",
    "                    #print(ln)\n",
    "\n",
    "\n",
    "            \"\"\"Team WAAAAAAEEEEE Cassie and Lance\"\"\"\n",
    "            if split_storm and is_wwlln:\n",
    "                ln_curr_timeframe = ln[(ln['Date'] >= date_val) & (ln['Date'] < date_val + datetime.timedelta(minutes=check_minutes))]\n",
    "                #print(ln_curr_timeframe)\n",
    "                if len(ln_curr_timeframe) != 0:\n",
    "                    ln_curr_timeframe['Distance'] = ln_curr_timeframe.apply(run_circle, axis = 1, args = ((center['Lat']), (center['Long'])))\n",
    "                    #print(ln_curr_timeframe)\n",
    "                    subset = ln_curr_timeframe[(ln_curr_timeframe['Distance'] <= 100 / 1.852) | ((ln_curr_timeframe['Distance'] >= 200 / 1.852) & (ln_curr_timeframe['Distance'] <= 400 / 1.852))]\n",
    "                else:\n",
    "                    subset = ln_curr_timeframe\n",
    "            else:\n",
    "                # Select just the lightning locations that are within (check_minutes) minutes\n",
    "                subset = ln[(ln['Date'] >= date_val) & (ln['Date'] < date_val + datetime.timedelta(minutes=check_minutes))]\n",
    "\n",
    "            \"\"\"Team WAAAAAAEEEEEE Cassie\"\"\"\n",
    "            if date_val >= right_half_dates[Tindex]:\n",
    "                Tindex += 1\n",
    "            stormTval = Tvalues[Tindex]\n",
    "            \n",
    "            # if lightning exists, do dbscan on it\n",
    "            if len(subset != 0):\n",
    "                #standard settings: eps=.2, min_samples=3\n",
    "                dbscan = DBSCAN(eps=.2, min_samples=3)\n",
    "                subset['Labels'] = dbscan.fit_predict(subset[['Long','Lat']])\n",
    "            \n",
    "            \"\"\"Team Squeem\"\"\"\n",
    "            # Setup the figure\n",
    "            if not disable_ploting:\n",
    "                fig, ax = plt.subplots(figsize=(15,15))\n",
    "            \n",
    "                # this is for the coastline\n",
    "                # This reads the shapefile, extracts each shape, creates a polygon, and adds it to the list\n",
    "                sf = shapefile.Reader(shp_file)\n",
    "                shapes = sf.shapes()\n",
    "                Nshp = len(shapes)\n",
    "                ptchs = []\n",
    "                for nshp in range(Nshp):\n",
    "                    pts = np.array(shapes[nshp].points)\n",
    "                    prt = shapes[nshp].parts\n",
    "                    par = list(prt) + [pts.shape[0]]\n",
    "                    for pij in range(len(prt)):\n",
    "                        ptchs.append(Polygon(pts[par[pij]:par[pij+1]]))\n",
    "                # Every every polygon from the shapefile to figure\n",
    "                ax.add_collection(PatchCollection(ptchs, facecolor= '#838688', edgecolor='k', linewidths=1., zorder=2))\n",
    "\n",
    "                # This adds the water color\n",
    "                ax.add_patch(mpl.patches.Rectangle((-180,-89),360,180,color='#a6cae0'))\n",
    "\n",
    "                # This connects Antarcitca to the bottom of the image\n",
    "                ax.add_patch(mpl.patches.Rectangle((-179.9,-89.9),360,2,color='#838688',zorder=3))\n",
    "\n",
    "                # These are more general image settings\n",
    "                if is_wwlln:\n",
    "                    dataset_name = \"WWLLN \"\n",
    "                else:\n",
    "                    dataset_name = \"GLM \"\n",
    "                plt.title(dataset_name + Storm_Names[dir_indx]+\"\\nT\"+str(stormTval)+' '+str(date_val), fontdict = {'fontsize' : 40})\n",
    "                #edges = (-180,180,-90,90) # Left, Right, Bottom, Top | Set this if you want to change the map scale\n",
    "                edges = (center['Long'].values[0] - buffer, center['Long'].values[0] + buffer, center['Lat'].values[0] - buffer, center['Lat'].values[0] + buffer) # <--------------------------This affects map zoooooooooom\n",
    "\n",
    "                # setting up img stuff, prob shouldn't be touched\n",
    "                xlim = np.append(np.arange(edges[0], edges[1], step=2.5), edges[1])\n",
    "                ylim = np.append(np.arange(edges[2], edges[3], step=2.5), edges[3])\n",
    "                ax.set_xticks(xlim)\n",
    "                ax.set_yticks(ylim)\n",
    "                ax.set_xlim(edges[0], edges[1])\n",
    "                ax.set_ylim(edges[2], edges[3])\n",
    "                ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "                ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "                plt.rc('xtick', labelsize=30)\n",
    "                plt.rc('ytick', labelsize=30)\n",
    "            \n",
    "            \"\"\"Team WAAAAAEEEEEE Cassie\"\"\"\n",
    "            # if lightning exists, put it in the image\n",
    "            \n",
    "            if len(subset) != 0 and not disable_ploting:\n",
    "                #print(subset)\n",
    "                plt.scatter(subset['Long'], subset['Lat'], c=subset['Labels'], cmap=cmap, zorder=4) # plots the dbscan data\n",
    "            \n",
    "            \"\"\"Team Squeem\"\"\"\n",
    "            if not disable_ploting:\n",
    "                #plt.scatter(subset['Long'],subset['Lat'],s=100,c=\"yellow\",edgecolors='black',zorder=5) # <- old plot func for non-grouped data\n",
    "                plt.scatter(center['Long'], center['Lat'], s=140, c=\"red\", edgecolors='black' ,zorder=5, alpha=0.5) # plots the center of the storm\n",
    "\n",
    "                # save and close the image\n",
    "                plt.savefig(Image_Path[dir_indx] + str(index)+\".png\", bbox_inches='tight', pad_inches=.4)\n",
    "            if split_storm and not disable_ploting:\n",
    "                if is_wwlln:\n",
    "                    shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", T_Path[dir_indx] + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "                    shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", \"Images/T Value/\" + str(stormTval) + '/' + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "                else:\n",
    "                    #shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", T_Path[dir_indx] + str(stormTval) + \"/glm_\" + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "                    shutil.copy(Image_Path[dir_indx] + str(index)+\".png\", \"Images/T Value/\" + str(stormTval) + \"/glm_\" + Storm_Names[dir_indx] + ' ' + str(index)+\".png\")\n",
    "            if not disable_ploting:\n",
    "                plt.close('all')\n",
    "\n",
    "            # Shows roughly how long we have left\n",
    "            taken = time.perf_counter() - starttime\n",
    "            print(f\"Storm #{dir_indx}: {Storm_Names[dir_indx]}, T{stormTval}\\tPercent Done: {(index + 1) / len(dates) * 100 : .2f}%\\tTime taken: {taken : .2f} seconds\\tEst time remaining: {datetime.timedelta(seconds=(len(dates) - (index + 1)) * taken)} {len(subset)}\", end='\\r')\n",
    "        dir_indx+=1\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Team WAAAAAAEEEEE Cassie\"\"\"\n",
    "\"\"\"This makes the GIFs\"\"\"\n",
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    with imageio.get_writer(Image_Path[dir_indx]+'All.gif', mode='I') as writer:\n",
    "        Images = [f for f in listdir(Image_Path[dir_indx]) if isfile(join(Image_Path[dir_indx], f)) and f.endswith(\".png\")]\n",
    "        img_cnt = len(Images)\n",
    "        img_indx = 0\n",
    "        while img_indx < img_cnt:\n",
    "            image = imageio.imread(Image_Path[dir_indx] + str(img_indx) + \".png\")\n",
    "            writer.append_data(image)\n",
    "            img_indx+=1\n",
    "    dir_indx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------#\n",
    "\" TESTING CELL \"\n",
    "#--------------#\n",
    "\"\"\"\n",
    "Useful for knowing what columns are in the dataset for coding\n",
    "appears 'printed' below box, has buttons that can be clicked to show/hide info\n",
    "\"\"\"\n",
    "from glmtools.io.glm import GLMDataset\n",
    "glm_data = GLMDataset(\"C:/Users/GamerPlayer1/Documents/Digipen/PersonalSVN/Spring22/CSP250/Storm_Data/GLM_setup/Dorian/234/00/OR_GLM-L2-LCFA_G16_s20192340000000_e20192340000200_c20192340000226.nc\").dataset\n",
    "glm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------#\n",
    "\" TESTING CELL \"\n",
    "#--------------#\n",
    "\"\"\"\n",
    "Generic testing code for how to interact with the dataset\n",
    "\"\"\"\n",
    "from glmtools.io.glm import GLMDataset\n",
    "glm_data = GLMDataset(\"C:/Users/GamerPlayer1/Documents/Digipen/PersonalSVN/Spring22/CSP250/Storm_Data/GLM_setup/Dorian/234/00/OR_GLM-L2-LCFA_G16_s20192340000000_e20192340000200_c20192340000226.nc\").dataset\n",
    "groups = glm_data[['group_energy','group_area']]\n",
    "flashes = glm_data[['flash_energy', 'flash_area']]\n",
    "# remove unused data (optional)\n",
    "# groups = groups.drop(['group_parent_flash_id','lightning_wavelength','product_time','group_time_threshold','flash_time_threshold','lat_field_of_view','lon_field_of_view'])\n",
    "# get the lattitude and longitude values\n",
    "group_lon, group_lat = groups['group_lon'].values, groups['group_lat'].values\n",
    "flash_lon, flash_lat = flashes['flash_lon'].values, flashes['flash_lat'].values\n",
    "\n",
    "(glm_data['number_of_groups'].values).size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
