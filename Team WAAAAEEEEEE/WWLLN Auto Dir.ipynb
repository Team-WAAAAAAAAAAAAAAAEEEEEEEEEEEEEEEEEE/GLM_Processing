{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # makes the notebook fill the whole window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import ticker\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Polygon\n",
    "from sklearn.cluster import DBSCAN\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import shapefile\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Turn off interactive plotting for pyplot\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Images/Hurricane_Dorian/', './Images/Hurricane_Laura/']\n",
      "['Data/Hurricane_Dorian/ATL_19_5_Dorian_Reduced_Trackfile.txt', 'Data/Hurricane_Laura/ATL_20_13_Laura_Reduced_Trackfile.txt']\n",
      "['Data/Hurricane_Dorian/ATL_19_5_Dorian_WWLLN_Locations.txt', 'Data/Hurricane_Laura/ATL_20_13_Laura_WWLLN_Locations.txt']\n",
      "['Data/Hurricane_Dorian/ATL_19_5_Dorian_Cubic_Spline_Trackfile.csv', 'Data/Hurricane_Laura/ATL_20_13_Laura_Cubic_Spline_Trackfile.csv']\n"
     ]
    }
   ],
   "source": [
    "only_dirs = [d for d in listdir(\"Data/\") if isdir(join(\"Data/\", d))]\n",
    "dir_count = len(only_dirs)\n",
    "WWLLN_Track_Path = [\"\"] * dir_count\n",
    "WWLLN_Locations_Path = [\"\"] * dir_count\n",
    "Cubic_Spline_Path = [\"\"] * dir_count\n",
    "Image_Path = [\"\"] * dir_count\n",
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    Image_Path[dir_indx] = join(\"./Images/\", only_dirs[dir_indx])\n",
    "    \n",
    "    if not os.path.exists(Image_Path[dir_indx]):\n",
    "        os.mkdir(Image_Path[dir_indx])\n",
    "    Image_Path[dir_indx] = Image_Path[dir_indx] + \"/\"\n",
    "    \n",
    "    only_dirs[dir_indx] = join(\"Data/\", only_dirs[dir_indx])\n",
    "    this_dir = only_dirs[dir_indx]\n",
    "    file_paths = listdir(this_dir)\n",
    "    \n",
    "    if len(file_paths) != 3:\n",
    "        only_dirs[dir_indx] = \"\"\n",
    "        continue\n",
    "    \n",
    "    #print(this_dir)\n",
    "    #print(file_paths)\n",
    "    file_indx = 0\n",
    "    while file_indx < 3:\n",
    "        file_paths[file_indx] = join(this_dir+\"/\", file_paths[file_indx])\n",
    "        #print(file_paths[file_indx])\n",
    "        \n",
    "        if file_paths[file_indx].endswith(\"Trackfile.txt\"):\n",
    "            WWLLN_Track_Path[dir_indx] = file_paths[file_indx]\n",
    "            #print(WWLLN_Track_Path)\n",
    "        elif file_paths[file_indx].endswith(\"Locations.txt\"):\n",
    "            WWLLN_Locations_Path[dir_indx] = file_paths[file_indx]\n",
    "            #print(WWLLN_Locations_Path)\n",
    "        elif file_paths[file_indx].endswith(\"Trackfile.csv\"):\n",
    "            Cubic_Spline_Path[dir_indx] = file_paths[file_indx]\n",
    "            #print(Cubic_Spline_Path)\n",
    "        file_indx+=1\n",
    "    #print('\\n')\n",
    "    dir_indx+=1\n",
    "print(Image_Path)\n",
    "print(WWLLN_Track_Path)\n",
    "print(WWLLN_Locations_Path)\n",
    "print(Cubic_Spline_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Done:  100.00%\tTime taken:  0.54 seconds\tEst time remaining: 0:00:00 22063 4752\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-c6cf988478f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdir_indx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mdir_indx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdir_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0monly_dirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdir_indx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dir_indx = 0\n",
    "while dir_indx < dir_count:\n",
    "    if only_dirs[dir_indx] == \"\":\n",
    "        continue\n",
    "    \n",
    "    # This loads in the WWLLN trackfile Jeremy and Natalia give you\n",
    "    df = pd.read_csv(WWLLN_Track_Path[dir_indx],header=None,names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Lat\",\"Long\",\"Min_Pressure\",\"Max_Winds\",\"Unused\"],low_memory=False,sep='\\t')\n",
    "    df = df.drop(\"Unused\",axis=1)\n",
    "    df['Date'] = df.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}\", format=\"%Y-%m-%d-%H\"),axis=1)\n",
    "\n",
    "    # This opens the WWLLN data that Jeremy and Natalia give you\n",
    "    ln = pd.read_csv(WWLLN_Locations_Path[dir_indx],header=None,names=[\"Year\",\"Month\",\"Day\",\"Hour\",\"Min\",\"Sec\",\"Lat\",\"Long\",\"Dist_East_West\",\"Dist_North_South\"],low_memory=False,sep=' ')\n",
    "    ln['Date'] = ln.apply(lambda x: pd.to_datetime(f\"{int(x['Year'])}-{int(x['Month'])}-{int(x['Day'])}-{int(x['Hour'])}-{int(x['Min'])}-{x['Sec']}\",format=\"%Y-%m-%d-%H-%M-%S.%f\"),axis=1)\n",
    "\n",
    "    # This opens the cubic splined trackfiles that I create\n",
    "    dorian_center = pd.read_csv(Cubic_Spline_Path[dir_indx])\n",
    "    dorian_center['Date'] = dorian_center['Date'].apply(pd.to_datetime)\n",
    "\n",
    "    # Genereates the list of dates from the WWLLN trackfile\n",
    "    dates = pd.date_range(df['Date'][0],df['Date'][len(df)-1],freq=\"10T\")\n",
    "\n",
    "    buffer = 10\n",
    "    shp_file = \"World_Countries__Generalized_-shp/World_Countries__Generalized_\"\n",
    "    cmap = 'gist_ncar'\n",
    "\n",
    "    # Counter and iteration\n",
    "    for index,date in enumerate(dates):\n",
    "        starttime = time.perf_counter()\n",
    "\n",
    "        # Select just the entries that are within 10 minutes\n",
    "        subset = ln[(ln['Date'] >= date) & (ln['Date'] < date + datetime.timedelta(minutes=10))]\n",
    "        center = dorian_center[dorian_center['Date'] == date]\n",
    "\n",
    "        if len(subset != 0):\n",
    "            dbscan = DBSCAN(eps=.2,min_samples=2)\n",
    "            subset['Labels'] = dbscan.fit_predict(subset[['Long','Lat']])\n",
    "\n",
    "        # Setup the figure\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "        # This reads the shapefile, extracts each shape, creates a polygon, and adds it to the list\n",
    "        sf = shapefile.Reader(shp_file)\n",
    "        shapes = sf.shapes()\n",
    "        Nshp = len(shapes)\n",
    "        ptchs = []\n",
    "        for nshp in range(Nshp):\n",
    "            pts = np.array(shapes[nshp].points)\n",
    "            prt = shapes[nshp].parts\n",
    "            par = list(prt) + [pts.shape[0]]\n",
    "            for pij in range(len(prt)):\n",
    "                ptchs.append(Polygon(pts[par[pij]:par[pij+1]]))\n",
    "        # Every every polygon from the shapefile to figure\n",
    "        ax.add_collection(PatchCollection(ptchs, facecolor= '#838688', edgecolor='k', linewidths=1., zorder=2))\n",
    "\n",
    "        # This adds the water color\n",
    "        ax.add_patch(mpl.patches.Rectangle((-180,-89),360,180,color='#a6cae0'))\n",
    "\n",
    "        # This connects Antarcitca to the bottom of the image\n",
    "        ax.add_patch(mpl.patches.Rectangle((-179.9,-89.9),360,2,color='#838688',zorder=3))\n",
    "\n",
    "        # These are more general image settings\n",
    "        plt.title(f\"Hurricane Dorian : {date}\")\n",
    "\n",
    "        #edges = (-180,180,-90,90) # Left, Right, Bottom, Top | Set this if you want to change the map scale\n",
    "        edges = (center['Long'].values[0] - buffer, center['Long'].values[0] + buffer, center['Lat'].values[0] - buffer, center['Lat'].values[0] + buffer)\n",
    "\n",
    "        xlim = np.append(np.arange(edges[0], edges[1], step=2.5), edges[1])\n",
    "        ylim = np.append(np.arange(edges[2], edges[3], step=2.5), edges[3])\n",
    "        ax.set_xticks(xlim)\n",
    "        ax.set_yticks(ylim)\n",
    "        ax.set_xlim(edges[0], edges[1])\n",
    "        ax.set_ylim(edges[2], edges[3])\n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "        if len(subset) != 0:\n",
    "            plt.scatter(subset['Long'], subset['Lat'], c=subset['Labels'], cmap=cmap, zorder=5)\n",
    "        #plt.scatter(subset['Long'],subset['Lat'],s=100,c=\"yellow\",edgecolors='black',zorder=5)\n",
    "        plt.scatter(center['Long'], center['Lat'], s=140, c=\"red\", edgecolors='black' ,zorder=4)\n",
    "\n",
    "        plt.savefig(Image_Path[dir_indx] + str(index), bbox_inches='tight', pad_inches=.4)\n",
    "        plt.close('all')\n",
    "\n",
    "        # Shows roughly how long we have left\n",
    "        taken = time.perf_counter() - starttime\n",
    "        print(f\"Percent Done: {(index + 1) / len(dates) * 100 : .2f}%\\tTime taken: {taken : .2f} seconds\\tEst time remaining: {datetime.timedelta(seconds=(len(dates) - (index + 1)) * taken)} {len(subset)}\",end='\\r')\n",
    "    dir_indx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
